{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUStack API \u306e\u4f7f\u7528\u4f8b\n",
    "\n",
    "\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001GPUStack\u306eOpenAI\u4e92\u63dbAPI\u3092\u4f7f\u7528\u3057\u3066\u3001\u30ed\u30fc\u30ab\u30eb\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u305fLLM\u30e2\u30c7\u30eb\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u57fa\u672c\u7684\u306a\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPUStack API\u306e\u8a2d\u5b9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# .env\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308b\u5834\u5408\u306f\u8aad\u307f\u8fbc\u3080\n",
    "load_dotenv()\n",
    "\n",
    "# GPUStack\u306eAPI\u8a2d\u5b9a\n",
    "GPUSTACK_API_BASE = os.getenv(\"GPUSTACK_API_BASE\", \"http://localhost:8000/v1\")\n",
    "GPUSTACK_API_KEY = os.getenv(\"GPUSTACK_API_KEY\", \"\")\n",
    "\n",
    "# API\u30ad\u30fc\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u3001\u624b\u52d5\u3067\u5165\u529b\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\n",
    "# from getpass import getpass\n",
    "# GPUSTACK_API_KEY = getpass(\"GPUStack\u306eAPI\u30ad\u30fc\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044: \")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPUStack\u304c\u5b9f\u884c\u4e2d\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def check_gpustack_running():\n",
    "    \"\"\"GPUStack\u304c\u5b9f\u884c\u4e2d\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{GPUSTACK_API_BASE}/models\")\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False\n",
    "\n",
    "if check_gpustack_running():\n",
    "    print(\"\u2705 GPUStack\u30b5\u30fc\u30d0\u30fc\u306b\u6b63\u5e38\u306b\u63a5\u7d9a\u3067\u304d\u307e\u3057\u305f\")\n",
    "else:\n",
    "    print(\"\u274c GPUStack\u30b5\u30fc\u30d0\u30fc\u306b\u63a5\u7d9a\u3067\u304d\u307e\u305b\u3093\u3002\u30b5\u30fc\u30d0\u30fc\u304c\u5b9f\u884c\u4e2d\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\u3002\")\n",
    "    print(\"   \u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3092\u5b9f\u884c\u3057\u3066GPUStack\u3092\u8d77\u52d5\u3057\u3066\u304f\u3060\u3055\u3044: gpustack start\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30c8\u3092\u53d6\u5f97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_models():\n",
    "    \"\"\"\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30c8\u3092\u53d6\u5f97\u3059\u308b\"\"\"\n",
    "    headers = {}\n",
    "    if GPUSTACK_API_KEY:\n",
    "        headers[\"Authorization\"] = f\"Bearer {GPUSTACK_API_KEY}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{GPUSTACK_API_BASE}/models\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"data\"]\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# \u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u30e2\u30c7\u30eb\u3092\u53d6\u5f97\n",
    "models = get_models()\n",
    "\n",
    "# \u30e2\u30c7\u30eb\u60c5\u5831\u3092DataFrame\u3067\u8868\u793a\n",
    "if models:\n",
    "    models_df = pd.DataFrame(models)\n",
    "    models_df[['id', 'status', 'type', 'device']]\n",
    "else:\n",
    "    print(\"\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u3066\u3044\u308b\u30e2\u30c7\u30eb\u304c\u3042\u308a\u307e\u305b\u3093\u3002\u30e2\u30c7\u30eb\u3092\u30c7\u30d7\u30ed\u30a4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenAI\u4e92\u63dbAPI\u3092\u4f7f\u7528\u3057\u3066LLM\u3068\u5bfe\u8a71\u3059\u308b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# \u5b9f\u884c\u4e2d\u306e\u30e2\u30c7\u30eb\u3092\u9078\u629e\n",
    "running_models = [model for model in models if model[\"status\"] == \"RUNNING\"]\n",
    "\n",
    "if not running_models:\n",
    "    print(\"\u5b9f\u884c\u4e2d\u306e\u30e2\u30c7\u30eb\u304c\u3042\u308a\u307e\u305b\u3093\u3002\u30e2\u30c7\u30eb\u304c\u8d77\u52d5\u3059\u308b\u306e\u3092\u5f85\u3064\u304b\u3001\u65b0\u3057\u3044\u30e2\u30c7\u30eb\u3092\u30c7\u30d7\u30ed\u30a4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\")\n",
    "else:\n",
    "    # \u6700\u521d\u306e\u5b9f\u884c\u4e2d\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n",
    "    model_id = running_models[0][\"id\"]\n",
    "    print(f\"\u4ee5\u4e0b\u306e\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3057\u307e\u3059: {model_id}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# OpenAI\u4e92\u63db\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306e\u4f5c\u6210\n",
    "client = OpenAI(\n",
    "    api_key=GPUSTACK_API_KEY or \"dummy_key\",  # API\u30ad\u30fc\u304c\u306a\u3044\u5834\u5408\u306f\u30c0\u30df\u30fc\u306e\u5024\u3092\u4f7f\u7528\n",
    "    base_url=GPUSTACK_API_BASE\n",
    ")\n",
    "\n",
    "# \u30e2\u30c7\u30eb\u3068\u30c1\u30e3\u30c3\u30c8\u3059\u308b\u95a2\u6570\n",
    "def chat_with_model(prompt, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\u3042\u306a\u305f\u306f\u5f79\u7acb\u3064AI\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {str(e)}\"\n",
    "\n",
    "# \u30c6\u30b9\u30c8\u30d7\u30ed\u30f3\u30d7\u30c8\n",
    "test_prompt = \"\u798f\u5ca1\u306e\u6709\u540d\u306a\u89b3\u5149\u30b9\u30dd\u30c3\u30c8\u30923\u3064\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u305d\u308c\u305e\u308c\u306b\u3064\u3044\u3066\u7c21\u5358\u306a\u8aac\u660e\u3082\u52a0\u3048\u3066\u304f\u3060\u3055\u3044\u3002\"\n",
    "\n",
    "# \u30e2\u30c7\u30eb\u304b\u3089\u5fdc\u7b54\u3092\u53d6\u5f97\n",
    "response = chat_with_model(test_prompt)\n",
    "print(response)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5909\u66f4\u3057\u3066\u307f\u308b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# \u5275\u9020\u6027\u306e\u9ad8\u3044\u5fdc\u7b54\uff08temperature=1.0\uff09\n",
    "creative_response = chat_with_model(\n",
    "    \"\u798f\u5ca1\u3092\u821e\u53f0\u306b\u3057\u305f\u30b7\u30e7\u30fc\u30c8\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002100\u5358\u8a9e\u7a0b\u5ea6\u3067\u7c21\u6f54\u306b\u3002\",\n",
    "    temperature=1.0\n",
    ")\n",
    "print(\"Temperature=1.0 (\u5275\u9020\u7684\u306a\u5fdc\u7b54):\")\n",
    "print(\"-\" * 80)\n",
    "print(creative_response)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# \u3088\u308a\u6c7a\u5b9a\u8ad6\u7684\u306a\u5fdc\u7b54\uff08temperature=0.2\uff09\n",
    "deterministic_response = chat_with_model(\n",
    "    \"\u798f\u5ca1\u3092\u821e\u53f0\u306b\u3057\u305f\u30b7\u30e7\u30fc\u30c8\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002100\u5358\u8a9e\u7a0b\u5ea6\u3067\u7c21\u6f54\u306b\u3002\",\n",
    "    temperature=0.2\n",
    ")\n",
    "print(\"\\nTemperature=0.2 (\u6c7a\u5b9a\u8ad6\u7684\u306a\u5fdc\u7b54):\")\n",
    "print(\"-\" * 80)\n",
    "print(deterministic_response)\n",
    "print(\"-\" * 80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. \u30c1\u30e3\u30c3\u30c8\u5c65\u6b74\u3092\u4f7f\u7528\u3057\u305f\u4f1a\u8a71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def chat_with_history(messages, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"\u30c1\u30e3\u30c3\u30c8\u5c65\u6b74\u3092\u4f7f\u7528\u3057\u3066\u30e2\u30c7\u30eb\u3068\u5bfe\u8a71\u3059\u308b\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {str(e)}\"\n",
    "\n",
    "# \u4f1a\u8a71\u5c65\u6b74\u306e\u521d\u671f\u5316\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"\u3042\u306a\u305f\u306f\u89aa\u5207\u306a\u30ac\u30a4\u30c9\u3067\u3059\u3002\u798f\u5ca1\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u3001\u5730\u5143\u306e\u4eba\u3057\u304b\u77e5\u3089\u306a\u3044\u3088\u3046\u306a\u60c5\u5831\u3082\u6559\u3048\u3066\u304f\u308c\u307e\u3059\u3002\"}\n",
    "]\n",
    "\n",
    "# \u6700\u521d\u306e\u8cea\u554f\n",
    "user_message = \"\u798f\u5ca1\u306b\u521d\u3081\u3066\u65c5\u884c\u3057\u307e\u3059\u3002\u304a\u3059\u3059\u3081\u306e\u5b63\u7bc0\u306f\u3044\u3064\u3067\u3059\u304b\uff1f\"\n",
    "conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "print(f\"\u30e6\u30fc\u30b6\u30fc: {user_message}\")\n",
    "\n",
    "# \u30e2\u30c7\u30eb\u306e\u5fdc\u7b54\n",
    "assistant_response = chat_with_history(conversation)\n",
    "conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "print(f\"\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8: {assistant_response}\")\n",
    "\n",
    "# \u30d5\u30a9\u30ed\u30fc\u30a2\u30c3\u30d7\u306e\u8cea\u554f\n",
    "user_message = \"\u305d\u306e\u5b63\u7bc0\u306b\u958b\u50ac\u3055\u308c\u308b\u5730\u5143\u306e\u304a\u796d\u308a\u3084\u884c\u4e8b\u306f\u3042\u308a\u307e\u3059\u304b\uff1f\"\n",
    "conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "print(f\"\\n\u30e6\u30fc\u30b6\u30fc: {user_message}\")\n",
    "\n",
    "# \u30e2\u30c7\u30eb\u306e\u5fdc\u7b54\n",
    "assistant_response = chat_with_history(conversation)\n",
    "conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "print(f\"\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8: {assistant_response}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JSON\u30e2\u30fc\u30c9\u3067\u306e\u51fa\u529b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def chat_with_json_mode(prompt, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"JSON\u5f62\u5f0f\u3067\u5fdc\u7b54\u3092\u53d6\u5f97\u3059\u308b\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\u3042\u306a\u305f\u306fJSON\u5f62\u5f0f\u3067\u5fdc\u7b54\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {str(e)}\"\n",
    "\n",
    "# JSON\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u30c7\u30fc\u30bf\u3092\u8981\u6c42\n",
    "json_prompt = \"\u798f\u5ca1\u306e\u6709\u540d\u306a3\u3064\u306e\u30e9\u30fc\u30e1\u30f3\u5e97\u306b\u3064\u3044\u3066\u3001\u5e97\u540d\u3001\u5834\u6240\u3001\u7279\u5fb4\u3092JSON\u5f62\u5f0f\u3067\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\"\n",
    "json_response = chat_with_json_mode(json_prompt)\n",
    "\n",
    "# \u5fdc\u7b54\u3092\u8868\u793a\n",
    "print(json_response)\n",
    "\n",
    "# JSON\u3092\u30d1\u30fc\u30b9\u3057\u3066DataFrame\u3067\u8868\u793a\n",
    "try:\n",
    "    data = json.loads(json_response)\n",
    "    if \"ramen_shops\" in data:\n",
    "        pd.DataFrame(data[\"ramen_shops\"])\n",
    "    else:\n",
    "        for key in data.keys():\n",
    "            if isinstance(data[key], list):\n",
    "                pd.DataFrame(data[key])\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"JSON\u306e\u30d1\u30fc\u30b9\u306b\u5931\u6557\u3057\u307e\u3057\u305f: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. \u5fdc\u7b54\u6642\u9593\u3068\u30c8\u30fc\u30af\u30f3\u6570\u306e\u6e2c\u5b9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# \u7570\u306a\u308b\u9577\u3055\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u30c6\u30b9\u30c8\n",
    "prompts = [\n",
    "    \"\u3053\u3093\u306b\u3061\u306f\",\n",
    "    \"\u798f\u5ca1\u306e\u7279\u7523\u54c1\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\",\n",
    "    \"\u798f\u5ca1\u306e\u6b74\u53f2\u3001\u6587\u5316\u3001\u89b3\u5149\u30b9\u30dd\u30c3\u30c8\u3001\u30b0\u30eb\u30e1\u3001\u4ea4\u901a\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u307e\u305f\u3001\u798f\u5ca1\u306b\u65c5\u884c\u3059\u308b\u969b\u306e\u304a\u3059\u3059\u3081\u30d7\u30e9\u30f3\u3082\u7d39\u4ecb\u3057\u3066\u304f\u3060\u3055\u3044\u3002\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # \u5fdc\u7b54\u6642\u9593\u3092\u6e2c\u5b9a\n",
    "    start_time = time.time()\n",
    "    response = chat_with_model(prompt, max_tokens=500)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # \u7d50\u679c\u3092\u8a18\u9332\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_length\": len(prompt),\n",
    "        \"response_length\": len(response),\n",
    "        \"time\": end_time - start_time\n",
    "    })\n",
    "\n",
    "# \u7d50\u679c\u3092DataFrame\u3067\u8868\u793a\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n",
    "# \u5fdc\u7b54\u6642\u9593\u306e\u30b0\u30e9\u30d5\u3092\u8868\u793a\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results_df[\"prompt_length\"], results_df[\"time\"])\n",
    "plt.xlabel(\"\u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u9577\u3055\uff08\u6587\u5b57\u6570\uff09\")\n",
    "plt.ylabel(\"\u5fdc\u7b54\u6642\u9593\uff08\u79d2\uff09\")\n",
    "plt.title(\"\u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u9577\u3055\u3068\u5fdc\u7b54\u6642\u9593\u306e\u95a2\u4fc2\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. \u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\u306e\u52b9\u679c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def chat_with_system_prompt(system_prompt, user_prompt, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"\u7570\u306a\u308b\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\u3067\u306e\u5fdc\u7b54\u3092\u6bd4\u8f03\u3059\u308b\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {str(e)}\"\n",
    "\n",
    "# \u30c6\u30b9\u30c8\u7528\u306e\u8cea\u554f\n",
    "test_question = \"\u798f\u5ca1\u306b\u3064\u3044\u3066\u6559\u3048\u3066\u304f\u3060\u3055\u3044\"\n",
    "\n",
    "# \u7570\u306a\u308b\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n",
    "system_prompts = [\n",
    "    \"\u3042\u306a\u305f\u306f\u7c21\u6f54\u306b\u7b54\u3048\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u77ed\u304f\u3001\u8981\u70b9\u3060\u3051\u3092\u4f1d\u3048\u3066\u304f\u3060\u3055\u3044\u3002\",\n",
    "    \"\u3042\u306a\u305f\u306f\u8a73\u3057\u304f\u8aac\u660e\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u8a73\u7d30\u306a\u60c5\u5831\u3068\u80cc\u666f\u77e5\u8b58\u3082\u542b\u3081\u3066\u56de\u7b54\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n",
    "    \"\u3042\u306a\u305f\u306f\u5730\u5143\u306e\u798f\u5ca1\u4eba\u306b\u306a\u308a\u304d\u3063\u3066\u4f1a\u8a71\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u65b9\u8a00\u3082\u4f7f\u3044\u3001\u89aa\u3057\u307f\u3084\u3059\u304f\u8a71\u3057\u3066\u304f\u3060\u3055\u3044\u3002\"\n",
    "]\n",
    "\n",
    "# \u5404\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\u3067\u306e\u5fdc\u7b54\u3092\u53d6\u5f97\n",
    "for i, system_prompt in enumerate(system_prompts, 1):\n",
    "    response = chat_with_system_prompt(system_prompt, test_question)\n",
    "    print(f\"\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8 {i}:\\n{system_prompt}\")\n",
    "    print(\"\\n\u5fdc\u7b54:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(response)\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. \u307e\u3068\u3081\n",
    "\n",
    "\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001GPUStack\u306eOpenAI\u4e92\u63dbAPI\u3092\u4f7f\u7528\u3057\u3066\u3001\u30ed\u30fc\u30ab\u30eb\u306b\u30c7\u30d7\u30ed\u30a4\u3055\u308c\u305fLLM\u30e2\u30c7\u30eb\u3092\u69d8\u3005\u306a\u65b9\u6cd5\u3067\u6d3b\u7528\u3059\u308b\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3057\u305f\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u3092\u5fdc\u7528\u3057\u3066\u3001\u72ec\u81ea\u306e\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3084\u30b5\u30fc\u30d3\u30b9\u3092\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n",
    "\n",
    "\u4e3b\u306a\u30dd\u30a4\u30f3\u30c8\uff1a\n",
    "1. OpenAI\u4e92\u63dbAPI\u3092\u4f7f\u3063\u3066\u30ed\u30fc\u30ab\u30eb\u306eLLM\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\n",
    "2. \u30c1\u30e3\u30c3\u30c8\u5c65\u6b74\u3092\u7528\u3044\u305f\u4f1a\u8a71\u306e\u5b9f\u73fe\n",
    "3. \u7570\u306a\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u8a2d\u5b9a\u306b\u3088\u308b\u5fdc\u7b54\u306e\u5909\u5316\n",
    "4. \u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u3088\u308b\u5fdc\u7b54\u30b9\u30bf\u30a4\u30eb\u306e\u5236\u5fa1\n",
    "5. JSON\u5f62\u5f0f\u3067\u306e\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u306e\u53d6\u5f97\n",
    "\n",
    "\u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u3001[GPUStack\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8](https://docs.gpustack.ai/)\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}