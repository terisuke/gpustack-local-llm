{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUStack API の使用例\n",
    "\n",
    "このノートブックでは、GPUStackのOpenAI互換APIを使用して、ローカルにデプロイされたLLMモデルとやり取りする基本的な方法を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPUStack APIの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .envファイルがある場合は読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# GPUStackのAPI設定\n",
    "GPUSTACK_API_BASE = os.getenv(\"GPUSTACK_API_BASE\", \"http://localhost:8000/v1\")\n",
    "GPUSTACK_API_KEY = os.getenv(\"GPUSTACK_API_KEY\", \"\")\n",
    "\n",
    "# APIキーが設定されていない場合は、手動で入力することもできます\n",
    "# from getpass import getpass\n",
    "# GPUSTACK_API_KEY = getpass(\"GPUStackのAPIキーを入力してください: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPUStackが実行中かどうかを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpustack_running():\n",
    "    \"\"\"GPUStackが実行中かどうかを確認する\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{GPUSTACK_API_BASE}/models\")\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False\n",
    "\n",
    "if check_gpustack_running():\n",
    "    print(\"✅ GPUStackサーバーに正常に接続できました\")\n",
    "else:\n",
    "    print(\"❌ GPUStackサーバーに接続できません。サーバーが実行中であることを確認してください。\")\n",
    "    print(\"   以下のコマンドを実行してGPUStackを起動してください: gpustack start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. デプロイされているモデルのリストを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \"\"\"デプロイされているモデルのリストを取得する\"\"\"\n",
    "    headers = {}\n",
    "    if GPUSTACK_API_KEY:\n",
    "        headers[\"Authorization\"] = f\"Bearer {GPUSTACK_API_KEY}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{GPUSTACK_API_BASE}/models\", headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"data\"]\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# デプロイされているモデルを取得\n",
    "models = get_models()\n",
    "\n",
    "# モデル情報をDataFrameで表示\n",
    "if models:\n",
    "    models_df = pd.DataFrame(models)\n",
    "    models_df[['id', 'status', 'type', 'device']]\n",
    "else:\n",
    "    print(\"デプロイされているモデルがありません。モデルをデプロイしてください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OpenAI互換APIを使用してLLMと対話する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行中のモデルを選択\n",
    "running_models = [model for model in models if model[\"status\"] == \"RUNNING\"]\n",
    "\n",
    "if not running_models:\n",
    "    print(\"実行中のモデルがありません。モデルが起動するのを待つか、新しいモデルをデプロイしてください。\")\n",
    "else:\n",
    "    # 最初の実行中モデルを使用\n",
    "    model_id = running_models[0][\"id\"]\n",
    "    print(f\"以下のモデルを使用します: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI互換クライアントの作成\n",
    "client = OpenAI(\n",
    "    api_key=GPUSTACK_API_KEY or \"dummy_key\",  # APIキーがない場合はダミーの値を使用\n",
    "    base_url=GPUSTACK_API_BASE\n",
    ")\n",
    "\n",
    "# モデルとチャットする関数\n",
    "def chat_with_model(prompt, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"あなたは役立つAIアシスタントです。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"エラーが発生しました: {str(e)}\"\n",
    "\n",
    "# テストプロンプト\n",
    "test_prompt = \"福岡の有名な観光スポットを3つ教えてください。それぞれについて簡単な説明も加えてください。\"\n",
    "\n",
    "# モデルから応答を取得\n",
    "response = chat_with_model(test_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. パラメータを変更してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創造性の高い応答（temperature=1.0）\n",
    "creative_response = chat_with_model(\n",
    "    \"福岡を舞台にしたショートストーリーを書いてください。100単語程度で簡潔に。\",\n",
    "    temperature=1.0\n",
    ")\n",
    "print(\"Temperature=1.0 (創造的な応答):\")\n",
    "print(\"-\" * 80)\n",
    "print(creative_response)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# より決定論的な応答（temperature=0.2）\n",
    "deterministic_response = chat_with_model(\n",
    "    \"福岡を舞台にしたショートストーリーを書いてください。100単語程度で簡潔に。\",\n",
    "    temperature=0.2\n",
    ")\n",
    "print(\"\\nTemperature=0.2 (決定論的な応答):\")\n",
    "print(\"-\" * 80)\n",
    "print(deterministic_response)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. チャット履歴を使用した会話"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_history(messages, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"チャット履歴を使用してモデルと対話する\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"エラーが発生しました: {str(e)}\"\n",
    "\n",
    "# 会話履歴の初期化\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは親切なガイドです。福岡について詳しく、地元の人しか知らないような情報も教えてくれます。\"}\n",
    "]\n",
    "\n",
    "# 最初の質問\n",
    "user_message = \"福岡に初めて旅行します。おすすめの季節はいつですか？\"\n",
    "conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "print(f\"ユーザー: {user_message}\")\n",
    "\n",
    "# モデルの応答\n",
    "assistant_response = chat_with_history(conversation)\n",
    "conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "print(f\"アシスタント: {assistant_response}\")\n",
    "\n",
    "# フォローアップの質問\n",
    "user_message = \"その季節に開催される地元のお祭りや行事はありますか？\"\n",
    "conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "print(f\"\\nユーザー: {user_message}\")\n",
    "\n",
    "# モデルの応答\n",
    "assistant_response = chat_with_history(conversation)\n",
    "conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "print(f\"アシスタント: {assistant_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JSONモードでの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_json_mode(prompt, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"JSON形式で応答を取得する\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"あなたはJSON形式で応答するアシスタントです。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"エラーが発生しました: {str(e)}\"\n",
    "\n",
    "# JSONフォーマットでデータを要求\n",
    "json_prompt = \"福岡の有名な3つのラーメン店について、店名、場所、特徴をJSON形式で教えてください。\"\n",
    "json_response = chat_with_json_mode(json_prompt)\n",
    "\n",
    "# 応答を表示\n",
    "print(json_response)\n",
    "\n",
    "# JSONをパースしてDataFrameで表示\n",
    "try:\n",
    "    data = json.loads(json_response)\n",
    "    if \"ramen_shops\" in data:\n",
    "        pd.DataFrame(data[\"ramen_shops\"])\n",
    "    else:\n",
    "        for key in data.keys():\n",
    "            if isinstance(data[key], list):\n",
    "                pd.DataFrame(data[key])\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"JSONのパースに失敗しました: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 応答時間とトークン数の測定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 異なる長さのプロンプトをテスト\n",
    "prompts = [\n",
    "    \"こんにちは\",\n",
    "    \"福岡の特産品について教えてください\",\n",
    "    \"福岡の歴史、文化、観光スポット、グルメ、交通について詳しく教えてください。また、福岡に旅行する際のおすすめプランも紹介してください。\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # 応答時間を測定\n",
    "    start_time = time.time()\n",
    "    response = chat_with_model(prompt, max_tokens=500)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 結果を記録\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_length\": len(prompt),\n",
    "        \"response_length\": len(response),\n",
    "        \"time\": end_time - start_time\n",
    "    })\n",
    "\n",
    "# 結果をDataFrameで表示\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n",
    "# 応答時間のグラフを表示\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(results_df[\"prompt_length\"], results_df[\"time\"])\n",
    "plt.xlabel(\"プロンプトの長さ（文字数）\")\n",
    "plt.ylabel(\"応答時間（秒）\")\n",
    "plt.title(\"プロンプトの長さと応答時間の関係\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. システムプロンプトの効果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_system_prompt(system_prompt, user_prompt, model=model_id, max_tokens=500, temperature=0.7):\n",
    "    \"\"\"異なるシステムプロンプトでの応答を比較する\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"エラーが発生しました: {str(e)}\"\n",
    "\n",
    "# テスト用の質問\n",
    "test_question = \"福岡について教えてください\"\n",
    "\n",
    "# 異なるシステムプロンプト\n",
    "system_prompts = [\n",
    "    \"あなたは簡潔に答えるアシスタントです。短く、要点だけを伝えてください。\",\n",
    "    \"あなたは詳しく説明するアシスタントです。詳細な情報と背景知識も含めて回答してください。\",\n",
    "    \"あなたは地元の福岡人になりきって会話してください。方言も使い、親しみやすく話してください。\"\n",
    "]\n",
    "\n",
    "# 各システムプロンプトでの応答を取得\n",
    "for i, system_prompt in enumerate(system_prompts, 1):\n",
    "    response = chat_with_system_prompt(system_prompt, test_question)\n",
    "    print(f\"システムプロンプト {i}:\\n{system_prompt}\")\n",
    "    print(\"\\n応答:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(response)\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. まとめ\n",
    "\n",
    "このノートブックでは、GPUStackのOpenAI互換APIを使用して、ローカルにデプロイされたLLMモデルを様々な方法で活用する方法を紹介しました。これらの手法を応用して、独自のアプリケーションやサービスを構築することができます。\n",
    "\n",
    "主なポイント：\n",
    "1. OpenAI互換APIを使ってローカルのLLMにアクセスできる\n",
    "2. チャット履歴を用いた会話の実現\n",
    "3. 異なるパラメータ設定による応答の変化\n",
    "4. システムプロンプトによる応答スタイルの制御\n",
    "5. JSON形式での構造化データの取得\n",
    "\n",
    "詳細については、[GPUStack公式ドキュメント](https://docs.gpustack.ai/)を参照してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}