#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPUStack ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
Streamlitã‚’ä½¿ç”¨ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªUI
"""

import os
import json
import time
import requests
from datetime import datetime
import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# GPUStackã®è¨­å®š
GPUSTACK_API_BASE = os.getenv("GPUSTACK_API_BASE", "http://localhost:8000/v1")
GPUSTACK_API_KEY = os.getenv("GPUSTACK_API_KEY", "")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.set_page_config(
    page_title="GPUStack ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
    page_icon="ğŸ¤–",
    layout="wide",
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ­ã‚´
st.sidebar.title("GPUStack ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.sidebar.image("https://raw.githubusercontent.com/sambanova/gpustack/main/docs/assets/GPUStack_Logo.png", width=200)
st.sidebar.markdown("---")

def check_gpustack_connection():
    """GPUStackã¨ã®æ¥ç¶šã‚’ç¢ºèªã™ã‚‹"""
    try:
        response = requests.get(f"{GPUSTACK_API_BASE}/models", timeout=5)
        return response.status_code == 200
    except requests.exceptions.ConnectionError:
        return False
    except requests.exceptions.Timeout:
        return False
    except Exception as e:
        st.error(f"GPUStackã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return False

def get_available_models():
    """ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
    headers = {}
    if GPUSTACK_API_KEY:
        headers["Authorization"] = f"Bearer {GPUSTACK_API_KEY}"
    
    try:
        response = requests.get(f"{GPUSTACK_API_BASE}/models", headers=headers)
        if response.status_code == 200:
            models_data = response.json()
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            models = [model["id"] for model in models_data["data"] if model["status"] == "RUNNING"]
            return models
        return []
    except:
        return []

def get_model_usage():
    """ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨çŠ¶æ³ã‚’å–å¾—ã™ã‚‹"""
    headers = {}
    if GPUSTACK_API_KEY:
        headers["Authorization"] = f"Bearer {GPUSTACK_API_KEY}"
    
    try:
        response = requests.get(f"{GPUSTACK_API_BASE}/metrics", headers=headers)
        if response.status_code == 200:
            return response.json()
        return None
    except:
        return None

def chat_with_model(model, messages, max_tokens=500, temperature=0.7, top_p=0.95):
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹"""
    client = OpenAI(
        api_key=GPUSTACK_API_KEY or "dummy_key",
        base_url=GPUSTACK_API_BASE
    )
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        return response.choices[0].message.content
    except requests.exceptions.ConnectionError:
        st.error("GPUStackã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šãŒåˆ‡æ–­ã•ã‚Œã¾ã—ãŸã€‚ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None
    except Exception as e:
        error_msg = str(e)
        if "æ¨¡å€£" in error_msg or "imitating" in error_msg:
            st.warning("ãƒ¢ãƒ‡ãƒ«ãŒãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ç”Ÿæˆã‚’åœæ­¢ã—ã¾ã—ãŸã€‚åˆ¥ã®è³ªå•ã‚’è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
        else:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}")
        return None

def init_session_state():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    if "model" not in st.session_state:
        st.session_state.model = None
    
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.7
    
    if "max_tokens" not in st.session_state:
        st.session_state.max_tokens = 500
    
    if "top_p" not in st.session_state:
        st.session_state.top_p = 0.95
    
    if "request_count" not in st.session_state:
        st.session_state.request_count = 0
    
    if "token_count" not in st.session_state:
        st.session_state.token_count = 0
    
    if "token_history" not in st.session_state:
        st.session_state.token_history = []

def display_chat_history():
    """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹"""
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def display_metrics():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤ºã™ã‚‹"""
    token_history = st.session_state.token_history
    
    if token_history:
        df = pd.DataFrame(token_history)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®ã‚°ãƒ©ãƒ•
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(df["timestamp"], df["tokens"], marker="o", linestyle="-", color="blue")
        ax.set_title("ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ¨ç§»")
        ax.set_xlabel("æ™‚é–“")
        ax.set_ylabel("ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
        ax.tick_params(axis="x", rotation=45)
        ax.grid(True)
        st.pyplot(fig)
        
        # çµ±è¨ˆæƒ…å ±
        st.markdown(f"**ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°:** {st.session_state.request_count}")
        st.markdown(f"**ç·ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡:** {st.session_state.token_count}")
        
        if len(df) > 1:
            avg_tokens = df["tokens"].mean()
            st.markdown(f"**å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:** {avg_tokens:.2f}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    init_session_state()
    
    # GPUStackã¨ã®æ¥ç¶šã‚’ç¢ºèªï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
    connected = False
    for i in range(3):  # æœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤
        if check_gpustack_connection():
            connected = True
            break
        if i < 2:  # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã§ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„
            st.info(f"GPUStackã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã‚’è©¦ã¿ã¦ã„ã¾ã™... ({i+1}/3)")
            time.sleep(2)
    
    if not connected:
        st.error("GPUStackã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.info("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦GPUStackã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
        st.code("gpustack start")
        return
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    available_models = get_available_models()
    
    if not available_models:
        st.warning("ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ãã ã•ã„ã€‚")
        st.info("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ãã ã•ã„:")
        st.code("python ../scripts/model_setup.py")
        return
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    with st.sidebar:
        st.header("ãƒ¢ãƒ‡ãƒ«è¨­å®š")
        
        selected_model = st.selectbox(
            "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
            available_models,
            index=0 if available_models else None
        )
        
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=st.session_state.temperature,
            step=0.1,
            help="å€¤ãŒé«˜ã„ã»ã©ã€ã‚ˆã‚Šå‰µé€ çš„ãªå¿œç­”ã«ãªã‚Šã¾ã™"
        )
        
        max_tokens = st.slider(
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            min_value=100,
            max_value=2000,
            value=st.session_state.max_tokens,
            step=100,
            help="ç”Ÿæˆã™ã‚‹å¿œç­”ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
        )
        
        top_p = st.slider(
            "Top P",
            min_value=0.1,
            max_value=1.0,
            value=st.session_state.top_p,
            step=0.05,
            help="ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã®ç¢ºç‡é–¾å€¤"
        )
        
        st.session_state.model = selected_model
        st.session_state.temperature = temperature
        st.session_state.max_tokens = max_tokens
        st.session_state.top_p = top_p
        
        st.markdown("---")
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š
        st.subheader("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
        system_prompt = st.text_area(
            "AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å½¹å‰²ã‚’è¨­å®š",
            value="ã‚ãªãŸã¯å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç°¡æ½”ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚",
            height=100
        )
        
        st.markdown("---")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
        st.subheader("ä½¿ç”¨çŠ¶æ³")
        display_metrics()
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã®ã‚¿ã‚¤ãƒˆãƒ«
    st.title("GPUStack ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    st.caption(f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«: {st.session_state.model}")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
    display_chat_history()
    
    # æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¥åŠ›
    user_input = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")
    
    if user_input:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ã‚’å–å¾—
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("è€ƒãˆä¸­...")
            
            # ãƒãƒ£ãƒƒãƒˆã®å±¥æ­´ã‚’ä½œæˆ
            history = [{"role": "system", "content": system_prompt}]
            for msg in st.session_state.messages:
                history.append({"role": msg["role"], "content": msg["content"]})
            
            # ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒƒãƒˆ
            start_time = time.time()
            response = chat_with_model(
                st.session_state.model,
                history,
                max_tokens=st.session_state.max_tokens,
                temperature=st.session_state.temperature,
                top_p=st.session_state.top_p
            )
            end_time = time.time()
            
            if response:
                message_placeholder.markdown(response)
                
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                st.session_state.messages.append({"role": "assistant", "content": response})
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
                st.session_state.request_count += 1
                
                # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¦‚ç®—ï¼ˆå®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¯APIã‹ã‚‰å–å¾—ã§ãã‚Œã°ç†æƒ³çš„ï¼‰
                # ç°¡æ˜“çš„ãªæ¨å®šã¨ã—ã¦ã€å˜èªæ•°ã®1.3å€ã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã—ã¦è¨ˆç®—
                estimated_tokens = int(len(response.split()) * 1.3)
                st.session_state.token_count += estimated_tokens
                
                st.session_state.token_history.append({
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "tokens": estimated_tokens,
                    "elapsed_time": end_time - start_time
                })

if __name__ == "__main__":
    main()
