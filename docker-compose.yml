version: '3.8'

services:
  gpustack:
    build:
      context: .
      dockerfile: Dockerfile
      # For multi-architecture builds with buildx
      platforms:
        - linux/amd64
        - linux/arm64
    ports:
      - "80:80"      # GPUStack Playground UI
      - "8501:8501"  # Streamlit app
    volumes:
      - gpustack_data:/var/lib/gpustack
    environment:
      - DEPLOY_MODEL=true
      # Specify which model to deploy (defaults to TinyLlama-1.1B if not set)
      - MODEL_ID=TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
    restart: unless-stopped

volumes:
  gpustack_data:
    # Persistent volume for model storage
